#! /usr/bin/env python
# -*- coding: utf-8 -*-

from __future__ import (absolute_import, division, print_function)

import os
import sys
import subprocess
import optparse
import six
if six.PY2:
    import MySQLdb
    import MySQLdb.cursors as cursors
else:
    import pymysql as MySQLdb
    import pymysql.cursors as cursors
import zlib
import time
import tempfile
import collections
import mpcsutil

sys.path.append(os.path.join( os.path.dirname(mpcsutil.__file__), 'internal'))
import eha_aggregation_pb2
from primitives import (time_primitives_pb2, eha_primitives_pb2, alarm_primitives_pb2)

currentMilliTime = lambda: int(round(time.time() * 1000))

# Constant definitions
SESSION_ID = 'sessionId'
HOST_ID = 'hostId'
CHANNEL_INDEX = 'channelIndex'
DSS_ID = 'dssId'
VC_ID = 'vcId'
CHANNEL_ID = 'channelId'
NAME = 'name'
TYPE = 'type'
DN_FORMAT = 'dnFormat'
EU_FORMAT = 'euFormat'
MODULE = 'module'
RCT_COARSE = 'rctCoarse'
RCT_FINE = 'rctFine'
PACKET_ID = 'packetId'
FRAME_ID = 'frameId'
PARENT_TYPE = 'parentType'
SCLK_COARSE = 'sclkCoarse'
SCLK_FINE = 'sclkFine'
ERT_COARSE = 'ertCoarse'
ERT_FINE = 'ertFine'
SCET_COARSE = 'scetCoarse'
SCET_FINE = 'scetFine'
FROM_SSE = 'fromSSE'
IS_REALTIME = 'isRealTime'
DN_UNSIGNED_VALUE = 'dnUnsignedValue'
DN_INTEGER_VALUE = 'dnIntegerValue'
DN_DOUBLE_VALUE = 'dnDoubleValue'
DN_DOUBLE_FLAG = 'dnDoubleFlag'
DN_STRING_VALUE = 'dnStringValue'
EU = 'eu'
EU_FLAG = 'euFlag'
DN_ALARM_STATE = 'dnAlarmState'
EU_ALARM_STATE = 'euAlarmState'
SESSION_FRAGMENT = 'sessionFragment'

BEGIN_RCT_COARSE = 'beginRctCoarse'
END_RCT_COARSE = 'endRctCoarse'
BEGIN_MST_COARSE = 'beginMstCoarse'
BEGIN_MST_FINE = 'beginMstFine'
END_MST_COARSE = 'endMstCoarse'
END_MST_FINE = 'endMstFine'
COUNT = 'count'
DISTINCT_COUNT = 'distinctCount'
CHAN_IDS_STRING = 'chanIdsString'

BEGIN_ERT_COARSE = 'beginErtCoarse'
BEGIN_ERT_FINE = 'beginErtFine'
END_ERT_COARSE = 'endErtCoarse'
END_ERT_FINE = 'endErtFine'
BEGIN_SCLK_COARSE = 'beginSclkCoarse'
END_SCLK_COARSE = 'endSclkCoarse'
BEGIN_SCET_COARSE = 'beginScetCoarse'
END_SCET_COARSE = 'endScetCoarse'
CHANNEL_TYPE = 'channelType'
PACKET_IDS = 'packetIds'
CONTENTS = 'contents'
APID = 'apid'


SESSION_RANGE = "<SESSION_RANGE>"
EXTENDED_SCET_SUFFIX = "<EXTENDED_SCET_SUFFIX>"
FSW_HEADER_FRAME_SCET_FINE = "<FSW_HEADER_FRAME_SCET_FINE>"

# SQL generated by R7.8 'chill_get_chanvals -K <session(s)> --channelTypes fr --sqlStatementOnly'
# The following strings: <SESSION_RANGE> and <EXTENDED_SCET_SUFFIX>
# get replaced based on execution configuration
FSW_SQL = '''SELECT cv.sessionId, cv.hostId, cd.channelIndex,
     cv.dssId, cv.vcid, cd.channelId, cd.name, cd.type, cd.dnFormat, cd.euFormat, cd.module,
     cv.rctCoarse, cv.rctFine,
     cv.packetId, NULL AS frameId, 'FSW' AS parentType,
     cv.sclkCoarse, cv.sclkFine, cv.ertCoarse as ertCoarse_mstCoarse,
     cv.ertFine as ertFine_mstFine, cv.scetCoarse, cv.scetFine, 0 AS fromSse,
     cv.isRealtime, cv.dnUnsignedValue, cv.dnIntegerValue,
     cv.dnDoubleValue, cv.dnDoubleFlag,
     cv.dnStringValue, cv.eu, cv.euFlag,
     cv.dnAlarmState, cv.euAlarmState, cv.sessionFragment
     FROM ChannelValue<EXTENDED_SCET_SUFFIX> AS cv
     STRAIGHT_JOIN ChannelData AS cd
     ON ((cv.sessionId=cd.sessionId) AND
         (cv.hostId=cd.hostId) AND
         (cv.sessionFragment=cd.sessionFragment) AND
         (0=cd.fromSse) AND
         (cv.id=cd.id)) WHERE (cv.sessionId IN (<SESSION_RANGE>))
'''

# SQL generated by R7.8 'chill_get_chanvals -K <session(s)> --channelTypes h --sqlStatementOnly'
# The following strings: <SESSION_RANGE>, <EXTENDED_SCET_SUFFIX> and <FSW_HEADER_FRAME_SCET_FINE>
# get replaced based on execution configuration
FSW_HEADER_SQL = '''SELECT hc.sessionId, hc.hostId, cd.channelIndex,
     hc.dssId, hc.vcid, cd.channelId, cd.name, cd.type, cd.dnFormat, cd.euFormat, NULL AS module,
     hc.rctCoarse, hc.rctFine,
     CONVERT(IF(hc.parentType='PACKET', hc.parentId, NULL), UNSIGNED INTEGER) AS packetId,
     CONVERT(IF(hc.parentType='FRAME',  hc.parentId, NULL), UNSIGNED INTEGER) AS frameId,
     hc.parentType,
     p.sclkCoarse, p.sclkFine,
     hc.ertCoarse as ertCoarse_mstCoarse,
     hc.ertFine as ertFine_mstFine,
     CONVERT(IF(hc.parentType='FRAME', hc.ertCoarse, p.scetCoarse), UNSIGNED INTEGER) AS scetCoarse,
     CONVERT(IF(hc.parentType='FRAME', hc.ertFine<FSW_HEADER_FRAME_SCET_FINE>,   p.scetFine),   UNSIGNED INTEGER) AS scetFine,
     hc.fromSse, 1 AS isRealtime,
     hc.dnUnsignedValue, hc.dnIntegerValue,
     hc.dnDoubleValue, hc.dnDoubleFlag,
     hc.dnStringValue,
     hc.eu, hc.euFlag, hc.dnAlarmState, hc.euAlarmState, hc.sessionFragment
     FROM HeaderChannelValue<EXTENDED_SCET_SUFFIX> AS hc
     STRAIGHT_JOIN ChannelData AS cd
     ON ((hc.sessionId=cd.sessionId) AND
         (hc.hostId=cd.hostId) AND
         (hc.sessionFragment=cd.sessionFragment) AND
         (hc.fromSse=0) AND
         (0=cd.fromSse) AND
         (hc.id=cd.id))
     LEFT JOIN Packet<EXTENDED_SCET_SUFFIX> AS p
     ON ((hc.sessionId=p.sessionId) AND
         (hc.hostId=p.hostId) AND
         (hc.sessionFragment=p.sessionFragment) AND
         (hc.parentType='PACKET') AND
         (hc.parentId=p.id)) WHERE (hc.sessionId IN (<SESSION_RANGE>))
'''

# SQL generated by 'chill_get_chanvals -K <session(s)> --channelTypes m --sqlStatementOnly'
# The following strings: <SESSION_RANGE> and <EXTENDED_SCET_SUFFIX>
# get replaced based on execution configuration
MONITOR_SQL = '''SELECT mc.sessionId, mc.hostId, cd.channelIndex,
     mc.dssId, NULL AS vcid, cd.channelId, cd.name, cd.type, cd.dnFormat, cd.euFormat, NULL AS module,
     mc.rctCoarse, mc.rctFine,
     NULL AS packetId, NULL AS frameId, 'MONITOR' AS parentType,
     NULL AS sclkCoarse, NULL AS sclkFine,
     mc.mstCoarse as ertCoarse_mstCoarse,
     mc.mstFine as ertFine_mstFine, NULL AS scetCoarse,
     NULL as scetFine, 0 AS fromSse, 1 AS isRealtime,
     mc.dnUnsignedValue, mc.dnIntegerValue,
     mc.dnDoubleValue, mc.dnDoubleFlag,
     mc.dnStringValue,
     mc.eu, mc.euFlag, mc.dnAlarmState, mc.euAlarmState, mc.sessionFragment
     FROM MonitorChannelValue AS mc
     STRAIGHT_JOIN ChannelData AS cd
     ON ((mc.sessionId=cd.sessionId) AND
         (mc.hostId=cd.hostId) AND
         (mc.sessionFragment=cd.sessionFragment) AND
         (0=cd.fromSse) AND
         (mc.id=cd.id)) WHERE (mc.sessionId IN (<SESSION_RANGE>))
'''

# SQL generated by 'chill_get_chanvals -K <session(s)> --channelTypes s --sqlStatementOnly'
# The following strings: <SESSION_RANGE> and <EXTENDED_SCET_SUFFIX>
# get replaced based on execution configuration
SSE_SQL = '''SELECT sv.sessionId, sv.hostId, cd.channelIndex,
     0 AS dssId, NULL AS vcid, cd.channelId, cd.name, cd.type, cd.dnFormat, cd.euFormat, cd.module,
     sv.rctCoarse, sv.rctFine,
     sv.packetId, NULL AS frameId, 'SSE' AS parentType,
     sv.sclkCoarse, sv.sclkFine, sv.ertCoarse as ertCoarse_mstCoarse,
     sv.ertFine as ertFine_mstFine, sv.scetCoarse, sv.scetFine, 1 AS fromSse,
     1 AS isRealtime, sv.dnUnsignedValue, sv.dnIntegerValue,
     sv.dnDoubleValue, sv.dnDoubleFlag,
     sv.dnStringValue, sv.eu, sv.euFlag, sv.dnAlarmState, sv.euAlarmState, sv.sessionFragment
     FROM SseChannelValue<EXTENDED_SCET_SUFFIX> AS sv
     STRAIGHT_JOIN ChannelData AS cd
     ON ((sv.sessionId=cd.sessionId) AND
         (sv.hostId=cd.hostId) AND
         (sv.sessionFragment=cd.sessionFragment) AND
         (1=cd.fromSse) AND
         (sv.id=cd.id)) WHERE (sv.sessionId IN (<SESSION_RANGE>))
'''

# SQL generated by 'chill_get_chanvals -K <session(s)> --channelTypes g --sqlStatementOnly'
# The following strings: <SESSION_RANGE> and <EXTENDED_SCET_SUFFIX>
# get replaced based on execution configuration
SSE_HEADER_SQL = '''SELECT hc.sessionId, hc.hostId, cd.channelIndex,
     0 AS dssId, NULL AS vcid, cd.channelId, cd.name, cd.type, cd.dnFormat, cd.euFormat, NULL AS module,
     hc.rctCoarse, hc.rctFine,
     hc.parentId AS packetId,
     NULL AS frameId,
     hc.parentType,
     p.sclkCoarse, p.sclkFine,
     hc.ertCoarse as ertCoarse_mstCoarse,
     hc.ertFine as ertFine_mstFine,
     p.scetCoarse, p.scetFine, hc.fromSse, 1 AS isRealtime,
     hc.dnUnsignedValue, hc.dnIntegerValue,
     hc.dnDoubleValue, hc.dnDoubleFlag,
     hc.dnStringValue,
     hc.eu, hc.euFlag, hc.dnAlarmState, hc.euAlarmState, hc.sessionFragment
     FROM HeaderChannelValue<EXTENDED_SCET_SUFFIX> AS hc
     STRAIGHT_JOIN ChannelData AS cd
     ON ((hc.sessionId=cd.sessionId) AND
         (hc.hostId=cd.hostId) AND
         (hc.sessionFragment=cd.sessionFragment) AND
         (hc.fromSse=1) AND
         (1=cd.fromSse) AND
         (hc.id=cd.id))
     LEFT JOIN SsePacket<EXTENDED_SCET_SUFFIX> AS p
     ON ((hc.sessionId=p.sessionId) AND
         (hc.hostId=p.hostId) AND
         (hc.sessionFragment=p.sessionFragment) AND
         (hc.parentId=p.id)) WHERE (hc.sessionId IN (<SESSION_RANGE>))
'''

# Database Column Names
columnNames = [
    SESSION_ID, HOST_ID, CHANNEL_INDEX, DSS_ID, VC_ID, CHANNEL_ID,
    NAME, TYPE, DN_FORMAT, EU_FORMAT, MODULE, RCT_COARSE, RCT_FINE,
    PACKET_ID, FRAME_ID, PARENT_TYPE, SCLK_COARSE, SCLK_FINE,ERT_COARSE,
    ERT_FINE, SCET_COARSE, SCET_FINE, FROM_SSE, IS_REALTIME, DN_UNSIGNED_VALUE,
    DN_INTEGER_VALUE, DN_DOUBLE_VALUE, DN_DOUBLE_FLAG, DN_STRING_VALUE, EU,
    EU_FLAG, DN_ALARM_STATE, EU_ALARM_STATE, SESSION_FRAGMENT]

MILLION_I = int(1000000)

# Default MAX for number of aggregates per LDI file
MAX_AGG_PER_LDI_FILE = MILLION_I

# Number of channel samples per aggregate
CHANNEL_SAMPLES_PER_AGGREGATE = 200

class DbTime:
    ''' Database Time object used for storing and comparing time infromation
    as its present in the DB.

    Object Attributes
    ------------------
    coarse - The Coarse portion of time as stored in the database
    fine - The Fine portion of time as stored in the database

    '''
    def __init__(self, coarse=None, fine=None):
        self.coarse = coarse
        self.fine = fine

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __gt__(self, other):
        if self.coarse > other.coarse:
            return True
        elif self.coarse == other.coarse:
            return self.fine > other.fine
        else:
            return False

    def __lt__(self, other):
        if self.coarse < other.coarse:
            return True
        elif self.coarse == other.coarse:
            return self.fine < other.fine
        else:
            return False

    def __str__(self):
        return 'coarse: %d, fine: %d' % (self.coarse, self.fine)

class EhaGroupDiscriminator():
    ''' EHA Group Discriminator object which is used to group channel
    samples that have the same metadata information.
    '''
    sessionId = 0
    hostId = 0
    sessionFragment = 0
    vcid = None
    apid = 0
    dssId = 0
    isRealtime = False
    isFromSSE = False
    channelType = ""

    def __str__(self):
        return str(self.__dict__)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __hash__(self):
        return hash((self.sessionId, self.hostId, self.sessionFragment, self.vcid, self.apid, self.dssId, self.isRealtime, self.isFromSSE, self.channelType))


class AggregateGroup():
    ''' Aggregate Group object which functions as an EHA Aggregate container

    Object Attributes
    ------------------
    group - The protobuf EhaAggregateGroup
    groupMemberCount - The number of channel samples contained in this group
    chanIdSet - The unique set of channel ids present in this group
    packetIdSet - The unique set of packet ids that are applicable to the samples of the group
    discriminator - The aggregate group discriminator
    keyValPair - The ChannelValue record dictionary containing the key, value pairs
    '''

    def __init__(self, discriminator=None, keyValPair=None):
        self.group = eha_aggregation_pb2.Proto3EhaAggregatedGroup()
        self.groupMemberCount = 0
        self.chanIdSet = set()
        self.packetIdSet = set()
        self.setDiscriminator(discriminator)
        self.setGroupTimeRange(keyValPair)
        self.addMember(keyValPair)

    def setDiscriminator(self, discriminator):
        self.discriminator = discriminator
        if discriminator.vcid is not None:
            self.group.discriminatorKey.vcid = discriminator.vcid
        self.group.discriminatorKey.dssId = discriminator.dssId
        self.group.discriminatorKey.isRealtime = discriminator.isRealtime
        self.group.discriminatorKey.isFromSSE = discriminator.isFromSSE

        if (discriminator.channelType == ChannelType.FSW_REC
            or discriminator.channelType == ChannelType.FSW_RT):
            self.group.discriminatorKey.chanType = eha_primitives_pb2.Proto3ChanDefType.Value('CHAN_DEF_TYPE_FSW')
        elif (discriminator.channelType == ChannelType.FSW_HEADER_FRAME
            or discriminator.channelType == ChannelType.FSW_HEADER_PACKET
            or discriminator.channelType == ChannelType.SSE_HEADER):
            self.group.discriminatorKey.chanType = eha_primitives_pb2.Proto3ChanDefType.Value('CHAN_DEF_TYPE_H')
        elif discriminator.channelType == ChannelType.MONITOR:
            self.group.discriminatorKey.chanType = eha_primitives_pb2.Proto3ChanDefType.Value('CHAN_DEF_TYPE_M')
        elif discriminator.channelType == ChannelType.SSE:
            self.group.discriminatorKey.chanType = eha_primitives_pb2.Proto3ChanDefType.Value('CHAN_DEF_TYPE_SSE')
        else:
            # Error, encountered unrecognized channelType
            print("Error: encountered unrecognized channelType")

    def setGroupTimeRange(self, keyValPair):
        self.groupTimeRange = AggregateGroupTimeRange(
            DbTime(keyValPair[ERT_COARSE], keyValPair[ERT_FINE]),
            DbTime(keyValPair[RCT_COARSE], keyValPair[RCT_FINE]),
            DbTime(keyValPair[SCLK_COARSE], keyValPair[SCLK_FINE]),
            DbTime(keyValPair[SCET_COARSE], keyValPair[SCET_FINE])
        )

    def addMember(self, keyValPair):
        self.addChannelId(keyValPair[CHANNEL_ID])
        self.addPacketId(keyValPair[PACKET_ID])
        self.addMemberToAggregateGroup(keyValPair)
        self.groupTimeRange.setCurrTimes(
            DbTime(keyValPair[ERT_COARSE], keyValPair[ERT_FINE]),
            DbTime(keyValPair[RCT_COARSE], keyValPair[RCT_FINE]),
            DbTime(keyValPair[SCLK_COARSE], keyValPair[SCLK_FINE]),
            DbTime(keyValPair[SCET_COARSE], keyValPair[SCET_FINE])
        )

    def addChannelId(self, chanId):
        self.chanIdSet.add(chanId)
        self.groupMemberCount += 1

    def addPacketId(self, packetId):
        if packetId is not None:
            self.packetIdSet.add(str(packetId))

    def addMemberToAggregateGroup(self, keyValPair):
        member = self.group.values.add()
        member.channelId = keyValPair[CHANNEL_ID]

        if keyValPair[SCLK_COARSE] is not None:
            member.sclk.seconds = keyValPair[SCLK_COARSE]
            member.sclk.nanos = keyValPair[SCLK_FINE]

        member.ert.milliseconds = int((keyValPair[ERT_COARSE] * 1000) + (keyValPair[ERT_FINE]/MILLION_I))
        member.ert.nanoseconds = (keyValPair[ERT_FINE] % MILLION_I)

        if keyValPair[SCET_COARSE] is not None:
            member.scet.milliseconds = int((keyValPair[SCET_COARSE] * 1000) + (keyValPair[SCET_FINE]/MILLION_I))
            member.scet.nanoseconds = (keyValPair[SCET_FINE] % MILLION_I)

        if keyValPair[NAME] is not None:
            member.name = keyValPair[NAME]

        if keyValPair[MODULE] is not None:
            member.module = keyValPair[MODULE]

        if keyValPair[DN_FORMAT] is not None:
            member.dnFormat = keyValPair[DN_FORMAT]

        if keyValPair[EU_FORMAT] is not None:
            member.euFormat = keyValPair[EU_FORMAT]

        if keyValPair[PACKET_ID] is not None:
            member.packetId = keyValPair[PACKET_ID]

        eu_val = self.getSafeDouble(keyValPair, eu=True)
        if eu_val is not None:
            member.eu = eu_val

        if keyValPair[TYPE] == 'UNSIGNED_INT':
            member.dn._ulong = keyValPair[DN_UNSIGNED_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_UNSIGNED_INT')
        elif keyValPair[TYPE] == 'TIME':
            member.dn._ulong = keyValPair[DN_UNSIGNED_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_TIME')
        elif keyValPair[TYPE] == 'DIGITAL':
            member.dn._ulong = keyValPair[DN_UNSIGNED_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_DIGITAL')
        elif keyValPair[TYPE] == 'SIGNED_INT':
            # If the DN is less than or equal to the max value of a Java integer 2^31-1
            # set the value as in INT, otherwise set it as a LONG
            if keyValPair[DN_INTEGER_VALUE] <= (2147483647):
                member.dn._int = int(keyValPair[DN_INTEGER_VALUE])
            else:
                member.dn._long = keyValPair[DN_INTEGER_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_SIGNED_INT')
        elif keyValPair[TYPE] == 'FLOAT':
            member.dn._double = self.getSafeDouble(keyValPair, eu=False)
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_FLOAT')
        elif keyValPair[TYPE] == 'STATUS':
            if keyValPair[DN_INTEGER_VALUE] is not None:
                member.dn._long = keyValPair[DN_INTEGER_VALUE]
            member.status = keyValPair[DN_STRING_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_STATUS')
        elif keyValPair[TYPE] == 'ASCII':
            member.dn._string = keyValPair[DN_STRING_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_ASCII')
        elif keyValPair[TYPE] == 'BOOLEAN':
            member.dn._bool = keyValPair[DN_UNSIGNED_VALUE]
            member.dn.type = eha_primitives_pb2.Proto3DnType.Value('DN_TYPE_BOOLEAN')
        else:
            sys.exit("Encountered an unknown type: %s" % keyValPair[TYPE])

        # Add DN Alarm info if its not NONE
        if keyValPair[DN_ALARM_STATE] != "NONE":
            alarm_value = member.alarmValueSet.alarms.add()
            alarm_value.isInAlarm = True
            alarm_value.isOnEu = False
            alarm_value.level = alarm_primitives_pb2.Proto3AlarmLevel.Value(keyValPair[DN_ALARM_STATE])

        # Add EU Alarm info if its not NONE
        if keyValPair[EU_ALARM_STATE] != "NONE":
            alarm_value = member.alarmValueSet.alarms.add()
            alarm_value.isInAlarm = True
            alarm_value.isOnEu = True
            alarm_value.level = alarm_primitives_pb2.Proto3AlarmLevel.Value(keyValPair[EU_ALARM_STATE])


        self.group.samples += 1

    def getSafeDouble(self, keyValPair, eu=False):

        val = keyValPair[EU] if eu else keyValPair[DN_DOUBLE_VALUE]

        if val is not None:
            # Not a NULL, so take the value as is
            return val

        # NULL, so check flag column

        flag = keyValPair[EU_FLAG] if eu else keyValPair[DN_DOUBLE_FLAG]

        if flag is None:
            # Not set, which means that extra information was not
            # stored, which means old data, so we use zero for DN.
            # EU can actually be legitimately NULL.
            return None if eu else 0.0

        if flag == 'WAS_PINFINITY':
            return float('inf')

        if flag == "WAS_NINFINITY":
            return float('-inf')

        if flag == "WAS_NAN":
            return float("nan")

        # Not what was expected; treat like null
        if eu:
            print("Unexpected EU flag value '" + flag + "'")
        else:
            print("Unexpected dnDouble flag value '" + flag + "'")

        return None if eu else 0.0

    def exactFromScetCoarseFine(self, coarse, fine):
        THOUSAND_L = long(1000)
        return ((coarse * THOUSAND_L) + (fine / MILLION_I))

    def fineFromScetFine(self, fine):
        return (fine % MILLION_I)



class AggregateGroupTimeRange():
    '''Container object for time range'''

    def __init__(self, currErt=None, currRct=None, currSclk=None, currScet=None):
        self.currErt = self.minErt = self.maxErt = currErt
        self.currRct = self.minRct = self.maxRct = currRct
        self.currSclk = self.minSclk = self.maxSclk = currSclk
        self.currScet = self.minScet = self.maxScet = currScet

    def setCurrTimes(self, currErt, currRct, currSclk, currScet):
        self.currErt = currErt
        self.currRct = currRct
        self.currSclk = currSclk
        self.currScet = currScet
        self.setMinMaxTimes()

    def setMinMaxTimes(self):
        if self.currErt > self.maxErt:
            self.maxErt = self.currErt

        if self.currErt < self.minErt:
            self.minErt = self.currErt

        if self.currRct > self.maxRct:
            self.maxRct = self.currRct

        if self.currRct < self.minRct:
            self.minRct = self.currRct

        if self.currSclk > self.maxSclk:
            self.maxSclk = self.currSclk

        if self.currSclk < self.minSclk:
            self.minSclk = self.currSclk

        if self.currScet > self.maxScet:
            self.maxScet = self.currScet

        if self.currScet < self.minScet:
            self.minScet = self.currScet

    def __str__(self):
        return str(self.__dict__)

    def __eq__(self, other):
        return self.__dict__ == other.__dict__

    def __hash__(self):
        return hash((self.currErt, self.currRct, self.currScet, self.currSclk,
            self.minErt, self.minRct, self.minScet, self.minSclk,
            self.maxErt, self.maxRct, self.maxScet, self.maxSclk))


class ChannelType:
    FSW = 'FSW'
    FSW_RT = 'FSW_RT'
    FSW_REC = 'FSW_REC'
    FSW_HEADER = 'FSW_HEADER'
    FSW_HEADER_FRAME = 'FSW_HEADER_FRAME'
    FSW_HEADER_PACKET = 'FSW_HEADER_PACKET'
    SSE = 'SSE'
    SSE_HEADER = 'SSE_HEADER'
    MONITOR = 'MONITOR'

# Map of Channel Types
channelTypeMap = {
    ChannelType.FSW : 'fr',
    ChannelType.FSW_HEADER : 'h',
    ChannelType.SSE : 's',
    ChannelType.SSE_HEADER : 'g',
    ChannelType.MONITOR : 'm'
}

class RecordIdProvider:
    '''Record Id provider which handles the record id generation'''

    def __init__(self):
        self.idCnt = 0

    def getNextId(self):
        self.idCnt +=1
        return self.idCnt

    def resetId(self):
        self.idCnt = 0


class OutputDataHandler:
    '''Output Data Handler used for writing LDI files'''

    def __init__(self, mission=None, outputDir=None, channelType=None):
        self.mission = mission
        self.ldiFileObj = None
        self.ldiFilePath = None
        self.outputDir = outputDir
        self.channelType = channelType
        self.filePartNumber = 1

    def openLdiFile(self):
        self.ldiFilePath = "{}/{}_channelType_{}_ldi_file_{}_part_{}.ldi".format(
            self.outputDir, self.mission, self.channelType, currentMilliTime(), self.filePartNumber)
        if six.PY2:
            self.ldiFileObj = open(self.ldiFilePath, "wb")
        else:
            self.ldiFileObj = open(self.ldiFilePath, "w", encoding='latin1')

    def incrementLdiFilePart(self):
        self.closeLdiFile()
        self.filePartNumber+=1
        self.openLdiFile()

    def writeLdiRecord(self, record=None):
        # if record:
        #     record = record.encode('latin1') if isinstance(record, six.string_types) else record
        # if record:
            # record = record.encode('utf8') if isinstance(record, six.string_types) else record
            # record = record.encode('latin1') if isinstance(record, six.string_types) else record

        # print('Record: {}'.format(record.decode('latin1')))
        self.ldiFileObj.write(record)

    def closeLdiFile(self):
        self.ldiFileObj.flush()
        self.ldiFileObj.close()

    def isClosed(self):
        return self.ldiFileObj.closed

    def getLdiFilePath(self):
        return self.ldiFilePath


class SqlFileHandler:
    '''SQL File Handler used for writing LDI LOAD SQL file which contains the commands for loading
    the LDI files'''

    LOAD_DATA_CONCURRENT = "LOAD DATA CONCURRENT LOCAL INFILE"

    FSW = ("INTO TABLE ChannelAggregate CHARACTER SET latin1 FIELDS TERMINATED BY ',' "
        "ESCAPED BY '\\\\' (hostId,sessionId,sessionFragment,id,channelType,packetIds,"
        "beginRctCoarse,endRctCoarse,beginErtCoarse,beginErtFine,endErtCoarse,endErtFine,"
        "beginSclkCoarse,endSclkCoarse,beginScetCoarse,endScetCoarse,dssId,vcid,count,"
        "distinctCount,chanIdsString,contents);")

    HEADER = ("INTO TABLE HeaderChannelAggregate CHARACTER SET latin1 FIELDS TERMINATED BY ',' "
        "ESCAPED BY '\\\\' (hostId,sessionId,sessionFragment,id,channelType,beginRctCoarse,"
        "endRctCoarse,beginErtCoarse,beginErtFine,endErtCoarse,endErtFine,apid,dssId,vcid,"
        "count,distinctCount,chanIdsString,contents);")

    MONITOR = ("INTO TABLE MonitorChannelAggregate CHARACTER SET latin1 FIELDS TERMINATED BY ',' "
        "ESCAPED BY '\\\\' (hostId,sessionId,sessionFragment,id,beginRctCoarse,endRctCoarse,"
        "beginMstCoarse,beginMstFine,endMstCoarse,endMstFine,dssId,count,distinctCount,"
        "chanIdsString,contents);")

    SSE = ("INTO TABLE SseChannelAggregate CHARACTER SET latin1 FIELDS TERMINATED BY ',' "
        "ESCAPED BY '\\\\' (hostId,sessionId,sessionFragment,id,packetIds,beginRctCoarse,"
        "endRctCoarse,beginErtCoarse,beginErtFine,endErtCoarse,endErtFine,beginSclkCoarse,"
        "endSclkCoarse,beginScetCoarse,endScetCoarse,count,distinctCount,chanIdsString,contents);")

    def __init__(self, mission=None, outputDir=None):
        self.mission = mission
        self.outputDir = outputDir
        self.sqlFileObj = None

    def openSqlFile(self):
        self.sqlFilePath = "{}/{}_ldi_load_{}.sql".format(self.outputDir, self.mission, currentMilliTime())
        self.sqlFileObj = open(self.sqlFilePath, "w")

    def writeLoadLdiCommand(self, channelType=None, ldiFilePath=None):
        fullLdiFilePath = os.path.abspath(ldiFilePath)
        tableName = ""
        if channelType == ChannelType.FSW:
            tableName = "ChannelAggregate"
            cmd = "{} '{}' {}\n".format(SqlFileHandler.LOAD_DATA_CONCURRENT,
                fullLdiFilePath, SqlFileHandler.FSW)
        elif channelType == ChannelType.FSW_HEADER or channelType == ChannelType.SSE_HEADER:
            tableName = "HeaderChannelAggregate"
            cmd = "{} '{}' {}\n".format(SqlFileHandler.LOAD_DATA_CONCURRENT,
                fullLdiFilePath, SqlFileHandler.HEADER)
        elif channelType == ChannelType.MONITOR:
            tableName = "MonitorChannelAggregate"
            cmd = "{} '{}' {}\n".format(SqlFileHandler.LOAD_DATA_CONCURRENT,
                fullLdiFilePath, SqlFileHandler.MONITOR)
        elif channelType == ChannelType.SSE:
            tableName = "SseChannelAggregate"
            cmd = "{} '{}' {}\n".format(SqlFileHandler.LOAD_DATA_CONCURRENT,
                fullLdiFilePath, SqlFileHandler.SSE)

        echoCmd = "system echo \"Loading into table: '{}' file: '{}'\"\n".format(tableName, fullLdiFilePath)
        self.sqlFileObj.write(echoCmd)
        self.sqlFileObj.write(cmd)
        self.sqlFileObj.flush()

    def closeSqlFile(self):
        self.sqlFileObj.flush()
        self.sqlFileObj.close()

def createCommandLineOptions():
    '''Create all the command line options for this application

    Returns
    --------
    An optparse.OptionParser object for parsing the command line arguments fed to this application'''

    parser = mpcsutil.create_option_parser()

    #optparse already automatically has its own --help and --version options
    parser.add_option("-o","--outputDirectory",action="callback",type="string",metavar="DIRECTORY",callback=parseOutputDir,
                      help="The directory where the output dump files should be placed.  Defaults to defined TMP directory")
    parser.add_option("-j","--databaseHost",action="callback",type="string",metavar="HOSTNAME",callback=parseDatabaseHost,
                      help="Host on which the database server is running")
    parser.add_option("-n","--databasePort",action="callback",type="int",metavar="INT",callback=parseDatabasePort,
                      help="Port on which the database server is running")
    parser.add_option("-u","--databaseUser",action="callback",type="string",metavar="USERNAME",callback=parseDatabaseUser,
                      help="Database user name")
    parser.add_option("-p","--databasePassword",action="callback",type="string",metavar="PASSWORD",callback=parseDatabasePwd,
                      help="Database password")
    parser.add_option("-N","--databaseName",action="callback",type="string",metavar="DATABASE_NAME",callback=parseDatabaseName,
                      help="Database name")
    parser.add_option("-K", "--testKey", action="callback", type="string", metavar="STRING", callback=parseTestKey,
                      help="Sessions")
    parser.add_option("-L","--maxAggPerLdiFile",action="callback",type="int",metavar="INT",callback=parseMaxAggPerLdiFile,
                      help="Maximum number of Aggregate records per LDI file.  Defaults to 1,000,000")

    parser.add_option("--extendedSCET",action="store_true",dest="extendedSCET",help="Query extended SCET tables")

    parser.add_option("--debug",action="store_true",dest="debugMode",help="Execute in debug mode")

    return parser


def parseDatabaseUser(option, opt, value, parser):
    '''A callback function for parsing the database username option

    Params
    -------
    option - The databaseUser Option instance
    opt - The long or short option value of the databaseUser option (that was used)
    value - The value of the databaseUser to use
    parser - is the OptionParser instance driving the whole thing'''

    if value is not None:
        value = value.lower().strip()

    parser.databaseUser = value

def parseDatabasePwd(option, opt, value, parser):
    '''A callback function for parsing the database password option

    Params
    -------
    option - The databasePassword Option instance
    opt - The long or short option value of the databasePassword option (that was used)
    value - The value of the databasePassword to use
    parser - is the OptionParser instance driving the whole thing'''

    if value is not None:
        value = value.strip()

    parser.databasePassword = value

def parseDatabaseHost(option, opt, value, parser):
    '''A callback function for parsing the database host option

    Params
    -------
    option - The databaseHost Option instance
    opt - The long or short option value of the databaseHost option (that was used)
    value - The value of the databaseHost to use
    parser - is the OptionParser instance driving the whole thing'''

    if value is not None:
        value = value.lower().strip()

    parser.databaseHost = value

def parseDatabasePort(option, opt, value, parser):
    '''A callback function for parsing the database port option

    Params
    -------
    option - The databasePort Option instance
    opt - The long or short option value of the databasePort option (that was used)
    value - The value of the databaseport to use
    parser - is the OptionParser instance driving the whole thing'''

    if value is not None:
        parser.databasePort = int(value)

def parseDatabaseName(option, opt, value, parser):
    '''A callback function for parsing the database name option

    Params
    -------
    option - The databaseName Option instance
    opt - The long or short option value of the databaseName option (that was used)
    value - The value of the databaseName to use
    parser - is the OptionParser instance driving the whole thing'''

    if not value:
        parser.error('Database Name is required')
        #raise optparse.OptionValueError('Database Name is required')

    if value is not None:
        value = value.lower().strip()

    parser.databaseName = value

def parseOutputDir(option, opt, value, parser):
    '''A callback function for parsing the output directory command line option

    Params
    -------
    option - The output directory Option instance
    opt - The long or short option value of the output directory option (that was used)
    value - The value of the output directory to use
    parser - is the OptionParser instance driving the whole thing'''

    if value is not None:
        value = value.strip()

    if not os.path.isdir(value):
        raise optparse.OptionValueError('Specified output directory "%s" does not exist' % (value))

    parser.outputDir = value


def parseTestKey(option, opt, value, parser):
    '''A callback function for parsing the testKey option.
       This parameter is a comma-separated list of session references.
       These are the kinds of session references we support:
           3       A single session
           5..10   A range of sessions

       The result is a set of fully expanded session range

    Params
    -------
    option - The dumpTables Option instance
    opt - The long or short option value of the dumpTables option (that was used)
    value - The value of the dumpTables to use
    parser - is the OptionParser instance driving the whole thing'''

    value = value.strip()

    if value is None or value == '':
        raise optparse.OptionValueError('No test keys supplied')

    results = set()

    for key in value.split(','):

        key = key.strip()

        part = key.split('..')

        if len(part) > 2:
            raise optparse.OptionValueError(
                        "Malformed --testKey option: Bad session range (%s)" % (key))

        first = parseInteger(part[0], 'session', False)

        if len(part) == 2:
            last = parseInteger(part[1], 'session', True)

        else:
            last = first

        if last != None and first > last:
            raise optparse.OptionValueError(
                        "Malformed --testKey option: Session ranges must be non-empty (%d..%d)" % (first, last))

        # We have an individual session
        if last == first:
            results.add(first)
        else:
            # We have a range of sessions
            # python range is not inclusive of the last value
            # so need to do last+1
            results.update(range(first, last+1))

    parser.testKey = value
    parser.sessionRange = results

def parseMaxAggPerLdiFile(option, opt, value, parser):
    '''A callback function for parsing the max aggregates per LDI file option

    Params
    -------
    option - The maximum number of aggregates per LDI file
    opt - The long or short option value of the maximum number of aggregates per LDI file (that was used)
    value - The value of the maximum number of aggregates per LDI file
    parser - is the OptionParser instance driving the whole thing'''

    if value is not None:
        parser.maxAggPerLdiFile = int(value)

def parseInteger(s, what, empty):
    '''Parse an integer from a string and validate as a session/fragment
       MPCS-5244 06/06/14 New method

    Params
    -------
    s     - The string to parse
    what  - The type of data for error messages
    empty - If true, allow empty to mean None

    Returns
    --------
    Integer parsed from the string or None'''

    s = s.strip()

    if empty and s == '':
        return None

    try:
        value = int(s)

        if value < 1:
            raise optparse.OptionValueError(
                      "Malformed --testKey option: %s must be > 0 (%d)" % (what, value))
    except ValueError:
        raise optparse.OptionValueError(
                  "Malformed --testKey option: Bad %s (%s)" % (what, s))

    return value


def parseCommandLine():
    '''Parse the command line options given to this application

    Returns
    --------
    A tuple of the form (mission,outputDirectory,sessionHost,startTimeLowerBound,startTimeUpperBound).  All values
    contained in the tuple are strings except the last two.'''

    parser = createCommandLineOptions()

    parser.outputDir = None
    parser.databaseHost = None
    parser.databasePort = None
    parser.databaseUser = None
    parser.databasePassword = None
    parser.databaseName = None
    parser.testKey = None
    parser.sessionRange = set()
    parser.maxAggPerLdiFile = MAX_AGG_PER_LDI_FILE

    (_options, _args) = parser.parse_args()

    mission = mpcsutil.config.GdsConfig().getMission()

    # Create a temporary output directory if its not specified
    if parser.outputDir is None:
        parser.outputDir = tempfile.gettempdir()

    try:
        os.makedirs(parser.outputDir)
    except OSError:
        pass

    extendedSCET = _options.extendedSCET
    if extendedSCET is None:
        extendedSCET = False

    debugMode = _options.debugMode
    if debugMode is None:
        debugMode = False

    return (mission,parser.outputDir,parser.databaseHost,parser.databasePort,\
            parser.databaseUser,parser.databasePassword,parser.databaseName,parser.testKey,\
            parser.sessionRange,parser.maxAggPerLdiFile,extendedSCET,debugMode)




class ConvHandler(object):
    @property
    def metadataTable(self):
        if self._metadataTable is None: self.metadataTable = {}
        return self._metadataTable
    @metadataTable.setter
    def metadataTable(self, value): self._metadataTable = value

    @property
    def querySqlMap(self):
        if self._querySqlMap is None: self.querySqlMap = self.buildSqlQueryMap()
        return self._querySqlMap
    @querySqlMap.setter
    def querySqlMap(self, value): self._querySqlMap = value

    @property
    def recordIdProvider(self):
        if self._recordIdProvider is None: self.recordIdProvider = RecordIdProvider()
        return self._recordIdProvider
    @recordIdProvider.setter
    def recordIdProvider(self, value): self._recordIdProvider = value

    def __init__(self, *args, **kwargs):
        [self.__setattr__(kk,vv) for kk, vv in kwargs.items()]
        self.conn=self._get_conn()

    def __getattr__(self, name):
        return self.__dict__.get(name)

    def __call__(self, *args, **kwargs):
        print("Starting data conversion...")
        self.startConversion()

    def _get_conn(self):
        # Initialize the DB connection object
        try:
            return MySQLdb.connect(
                host=self.databaseHost,port=self.databasePort,
                user=self.databaseUser,passwd=self.databasePassword,
                db=self.databaseName, cursorclass=cursors.SSCursor, charset='utf8')

        except MySQLdb.Error as e:
            print("Encountered unexpected MariaDb error:")
            raise
            # sys.exit(e)

    def buildSqlQueryMap(self):
        '''Builds the SQL Query Map for each of the different
        Channel Types (--channelType options 'fr, h, m, s, g')

        Params
        -------
        sessionSet - A set containing fully expanded session numbers
        extendedSCET - A flag indicating whether or not we are querying Extended SCET tables

        Returns
        --------
        A dictinary containing the SQL commands for each channel type'''

        sqlQueryMap = {}

        sessionRange = ",".join(str(s) for s in self.sessionRange)
        extendedScetSuffix = ""
        fswHeaderFrameScetFine = " / 1000000"
        if self.extendedSCET:
            extendedScetSuffix = "2"
            fswHeaderFrameScetFine = ""

        # Build the full SQL commands by filling in the SESSION_RANGE
        # and Extended EXTENDED_SCET_SUFFIX
        # FSW_HEADER_FRAME_SCET_FINE is only applicable to FSW HEADER
        for chanType,queryType in channelTypeMap.items():
            if chanType == ChannelType.FSW:
                sqlCmd = FSW_SQL
                sqlCmd = sqlCmd.replace(SESSION_RANGE, sessionRange)
                sqlCmd = sqlCmd.replace(EXTENDED_SCET_SUFFIX, extendedScetSuffix)
            elif chanType == ChannelType.FSW_HEADER:
                sqlCmd = FSW_HEADER_SQL
                sqlCmd = sqlCmd.replace(SESSION_RANGE, sessionRange)
                sqlCmd = sqlCmd.replace(EXTENDED_SCET_SUFFIX, extendedScetSuffix)
                sqlCmd = sqlCmd.replace(FSW_HEADER_FRAME_SCET_FINE, fswHeaderFrameScetFine)
            elif chanType == ChannelType.MONITOR:
                sqlCmd = MONITOR_SQL
                sqlCmd = sqlCmd.replace(SESSION_RANGE, sessionRange)
                sqlCmd = sqlCmd.replace(EXTENDED_SCET_SUFFIX, extendedScetSuffix)
            elif chanType == ChannelType.SSE:
                sqlCmd = SSE_SQL
                sqlCmd = sqlCmd.replace(SESSION_RANGE, sessionRange)
                sqlCmd = sqlCmd.replace(EXTENDED_SCET_SUFFIX, extendedScetSuffix)
            elif chanType == ChannelType.SSE_HEADER:
                sqlCmd = SSE_HEADER_SQL
                sqlCmd = sqlCmd.replace(SESSION_RANGE, sessionRange)
                sqlCmd = sqlCmd.replace(EXTENDED_SCET_SUFFIX, extendedScetSuffix)

            # Add the type : sql entry to the map
            sqlQueryMap.update({chanType: sqlCmd})

        return sqlQueryMap


    def prepareAggregateBlob(self, aggregateGroup):
        '''Prepare the aggregate BLOB for insertion into the database.
        Protobuf structure is serialized, compressed and the binary BLOB ran
        through the MySQL escape_string() function

        Params
        -------
        aggregateGroup - The aggregate group

        Returns
        --------
        The prepared aggregate BLOB'''


        # Serialize Aggregate
        serializedAggregateGroup = aggregateGroup.SerializeToString()
        # print('SerializeToString Type: {}'.format(type(serializedAggregateGroup)))
        # return self.conn.escape_string(serializedAggregateGroup.decode('utf-8'))
        # _serializedAggregateGroup=serializedAggregateGroup.decode('latin1').encode('utf8')
        # print('SerializeToString: {}'.format(_serializedAggregateGroup))
        # compressedBlob = zlib.compress(serializedAggregateGroup, 1)
        # return MySQLdb.converters.escape_bytes(compressedBlob)

        compressedBlob = zlib.compress(serializedAggregateGroup, 1).decode('latin1')
        return self.conn.escape_string(compressedBlob)


        # return MySQLdb.converters.escape_bytes(serializedAggregateGroup)
        # Compress
        compressedBlob = zlib.compress(serializedAggregateGroup, 1)
        # Escape
        if six.PY2:
            commaEscapedCompressedBlob = self.conn.escape_string(compressedBlob)
        else:
            commaEscapedCompressedBlob = MySQLdb.converters.escape_bytes(compressedBlob)

        return commaEscapedCompressedBlob


    def setCommonMetadata(self, metadataTable, aggregateGroup):
        '''Set up common LDI record metadata fields

        Params
        -------
        metadataTable - The list which will hold the metadata information
        aggregateGroup - The aggregate group'''

        metadataTable[HOST_ID] = aggregateGroup.discriminator.hostId
        metadataTable[SESSION_ID] = aggregateGroup.discriminator.sessionId
        metadataTable[SESSION_FRAGMENT] = aggregateGroup.discriminator.sessionFragment

        metadataTable[BEGIN_RCT_COARSE] = aggregateGroup.groupTimeRange.minRct.coarse
        metadataTable[END_RCT_COARSE] = aggregateGroup.groupTimeRange.maxRct.coarse
        metadataTable[BEGIN_ERT_COARSE] = aggregateGroup.groupTimeRange.minErt.coarse
        metadataTable[BEGIN_ERT_FINE] = aggregateGroup.groupTimeRange.minErt.fine
        metadataTable[END_ERT_COARSE] = aggregateGroup.groupTimeRange.maxErt.coarse
        metadataTable[END_ERT_FINE] = aggregateGroup.groupTimeRange.maxErt.fine

        metadataTable[COUNT] = aggregateGroup.group.samples
        metadataTable[DISTINCT_COUNT] = len(aggregateGroup.chanIdSet)
        metadataTable[CHAN_IDS_STRING] = ":".join(sorted(aggregateGroup.chanIdSet))


    def setAggregateMetadata(self, metadataTable, aggregateGroup, set_sclk_scet=False):
        '''Set aggregate group metadata fields in protobuf structure

        Params
        -------
        metadataTable - The list which provides the metadata information
        aggregateGroup - The aggregate group

        Returns
        --------
        The updated aggregate group'''

        group = aggregateGroup.group

        # Set Aggregate Group ERT Range
        group.ertRange.min.milliseconds = metadataTable[BEGIN_ERT_COARSE]
        group.ertRange.min.nanoseconds = metadataTable[BEGIN_ERT_FINE]
        group.ertRange.max.milliseconds = metadataTable[END_ERT_COARSE]
        group.ertRange.max.nanoseconds = metadataTable[END_ERT_FINE]

        # Set Aggregate Group RCT Range
        group.rctRange.min.milliseconds = metadataTable[BEGIN_RCT_COARSE]
        group.rctRange.min.nanoseconds = aggregateGroup.groupTimeRange.minRct.fine
        group.rctRange.max.milliseconds = metadataTable[END_RCT_COARSE]
        group.rctRange.max.nanoseconds = aggregateGroup.groupTimeRange.maxRct.fine

        if set_sclk_scet:
            metadataTable[BEGIN_SCLK_COARSE] = aggregateGroup.groupTimeRange.minSclk.coarse
            metadataTable[END_SCLK_COARSE] = aggregateGroup.groupTimeRange.maxSclk.coarse
            metadataTable[BEGIN_SCET_COARSE] = aggregateGroup.groupTimeRange.minScet.coarse
            metadataTable[END_SCET_COARSE] = aggregateGroup.groupTimeRange.maxScet.coarse

            # Set Aggregate Group SCLK Range
            if metadataTable[BEGIN_SCLK_COARSE] is not None and metadataTable[END_SCLK_COARSE] is not None:
                group.sclkRange.min.seconds = metadataTable[BEGIN_SCLK_COARSE]
                group.sclkRange.min.nanos = aggregateGroup.groupTimeRange.minSclk.fine
                group.sclkRange.max.seconds = metadataTable[END_SCLK_COARSE]
                group.sclkRange.max.nanos = aggregateGroup.groupTimeRange.maxSclk.fine

            # Set Aggregate Group SCET Range
            if metadataTable[BEGIN_SCET_COARSE] is not None and metadataTable[END_SCET_COARSE] is not None:
                group.scetRange.min.milliseconds = metadataTable[BEGIN_SCET_COARSE]
                group.scetRange.min.nanoseconds = aggregateGroup.groupTimeRange.minScet.fine
                group.scetRange.max.milliseconds = metadataTable[END_SCET_COARSE]
                group.scetRange.max.nanoseconds = aggregateGroup.groupTimeRange.maxScet.fine

        return group


    def getChannelAggregateLdiRecord(self, aggregateGroup):
        '''Get the ChannelAggregate table LDI record

        Params
        -------
        aggregateGroup - The aggregate group

        Returns
        --------
        The ChannelAggregate LDI record'''

        metadataTable = {}
        self.setCommonMetadata(metadataTable, aggregateGroup)

        chanType = aggregateGroup.discriminator.channelType
        if chanType == ChannelType.FSW_RT:
            metadataTable[CHANNEL_TYPE] = "FSW_RT"
        elif chanType == ChannelType.FSW_REC:
            metadataTable[CHANNEL_TYPE] = "FSW_REC"

        metadataTable[DSS_ID] = aggregateGroup.discriminator.dssId
        metadataTable[VC_ID] = aggregateGroup.discriminator.vcid

        # When the VCID is python None, need to set the VCID field to "\N"
        # in the LDI file so that MariaDB will load the field to NULL,
        # otherwise the None is interpreted as 0
        if metadataTable[VC_ID] is None:
            metadataTable[VC_ID] = b"\N".decode()

        metadataTable[PACKET_IDS] = ":".join(aggregateGroup.packetIdSet)

        group = self.setAggregateMetadata(metadataTable, aggregateGroup, set_sclk_scet=True)

        metadataTable[CONTENTS] = self.prepareAggregateBlob(group)

        ldiRecord = "{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},".format(
            metadataTable[HOST_ID],
            metadataTable[SESSION_ID],
            metadataTable[SESSION_FRAGMENT],
            self.recordIdProvider.getNextId(),
            metadataTable[CHANNEL_TYPE],
            metadataTable[PACKET_IDS],
            metadataTable[BEGIN_RCT_COARSE],
            metadataTable[END_RCT_COARSE],
            metadataTable[BEGIN_ERT_COARSE],
            metadataTable[BEGIN_ERT_FINE],
            metadataTable[END_ERT_COARSE],
            metadataTable[END_ERT_FINE],
            metadataTable[BEGIN_SCLK_COARSE],
            metadataTable[END_SCLK_COARSE],
            metadataTable[BEGIN_SCET_COARSE],
            metadataTable[END_SCET_COARSE],
            metadataTable[DSS_ID],
            metadataTable[VC_ID],
            metadataTable[COUNT],
            metadataTable[DISTINCT_COUNT],
            metadataTable[CHAN_IDS_STRING]
        )
        ldiRecord += metadataTable[CONTENTS].replace(',', '\\,')
        return ldiRecord

    def getHeaderChannelAggregateLdiRecord(self, aggregateGroup):
        '''Get the HeaderChannelAggregate table LDI record

        Params
        -------
        aggregateGroup - The aggregate group

        Returns
        --------
        The HeaderChannelAggregate LDI record'''

        metadataTable = {}
        self.setCommonMetadata(metadataTable, aggregateGroup)

        chanType = aggregateGroup.discriminator.channelType
        if chanType == ChannelType.FSW_HEADER_FRAME:
            metadataTable[CHANNEL_TYPE] = "FRAME_HEADER"
        elif chanType == ChannelType.FSW_HEADER_PACKET:
            metadataTable[CHANNEL_TYPE] = "PACKET_HEADER"
        elif chanType == ChannelType.SSE_HEADER:
            metadataTable[CHANNEL_TYPE] = "SSE_HEADER"

        # TODO: Figure out if we need to have APID for HeaderChannels
        metadataTable[APID] = 0
        metadataTable[DSS_ID] = aggregateGroup.discriminator.dssId

        if aggregateGroup.discriminator.vcid is not None:
            metadataTable[VC_ID] = aggregateGroup.discriminator.vcid
        else:
            metadataTable[VC_ID] = "\\N"

        group = self.setAggregateMetadata(metadataTable, aggregateGroup, set_sclk_scet=False)

        metadataTable[CONTENTS] = self.prepareAggregateBlob(group)

        ldiRecord = "{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},".format(
            metadataTable[HOST_ID],
            metadataTable[SESSION_ID],
            metadataTable[SESSION_FRAGMENT],
            self.recordIdProvider.getNextId(),
            metadataTable[CHANNEL_TYPE],
            metadataTable[BEGIN_RCT_COARSE],
            metadataTable[END_RCT_COARSE],
            metadataTable[BEGIN_ERT_COARSE],
            metadataTable[BEGIN_ERT_FINE],
            metadataTable[END_ERT_COARSE],
            metadataTable[END_ERT_FINE],
            metadataTable[APID],
            metadataTable[DSS_ID],
            metadataTable[VC_ID],
            metadataTable[COUNT],
            metadataTable[DISTINCT_COUNT],
            metadataTable[CHAN_IDS_STRING]
        )
        ldiRecord += metadataTable[CONTENTS].replace(',', '\\,')
        return ldiRecord

    def getSseChannelAggregateLdiRecord(self, aggregateGroup):
        '''Get the SseChannelAggregate table LDI record

        Params
        -------
        aggregateGroup - The aggregate group

        Returns
        --------
        The SseChannelAggregate LDI record'''

        metadataTable = {}
        self.setCommonMetadata(metadataTable, aggregateGroup)

        metadataTable[PACKET_IDS] = ":".join(aggregateGroup.packetIdSet)

        metadataTable[BEGIN_SCLK_COARSE] = aggregateGroup.groupTimeRange.minSclk.coarse
        metadataTable[END_SCLK_COARSE] = aggregateGroup.groupTimeRange.maxSclk.coarse
        metadataTable[BEGIN_SCET_COARSE] = aggregateGroup.groupTimeRange.minScet.coarse
        metadataTable[END_SCET_COARSE] = aggregateGroup.groupTimeRange.maxScet.coarse

        group = self.setAggregateMetadata(metadataTable, aggregateGroup, set_sclk_scet=True)

        metadataTable[CONTENTS] = self.prepareAggregateBlob(group)

        ldiRecord = "{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},{},".format(
            metadataTable[HOST_ID],
            metadataTable[SESSION_ID],
            metadataTable[SESSION_FRAGMENT],
            self.recordIdProvider.getNextId(),
            metadataTable[PACKET_IDS],
            metadataTable[BEGIN_RCT_COARSE],
            metadataTable[END_RCT_COARSE],
            metadataTable[BEGIN_ERT_COARSE],
            metadataTable[BEGIN_ERT_FINE],
            metadataTable[END_ERT_COARSE],
            metadataTable[END_ERT_FINE],
            metadataTable[BEGIN_SCLK_COARSE],
            metadataTable[END_SCLK_COARSE],
            metadataTable[BEGIN_SCET_COARSE],
            metadataTable[END_SCET_COARSE],
            metadataTable[COUNT],
            metadataTable[DISTINCT_COUNT],
            metadataTable[CHAN_IDS_STRING]
        )
        ldiRecord += metadataTable[CONTENTS].replace(',', '\\,')
        return ldiRecord

    def getMonitorChannelAggregateLdiRecord(self, aggregateGroup):
        '''Get the MonitorChannelAggregate table LDI record

        Params
        -------
        aggregateGroup - The aggregate group

        Returns
        --------
        The MonitorChannelAggregate LDI record'''

        metadataTable = {}
        self.setCommonMetadata(metadataTable, aggregateGroup)

        metadataTable[DSS_ID] = aggregateGroup.discriminator.dssId
        metadataTable[PACKET_IDS] = ":".join(aggregateGroup.packetIdSet)

        metadataTable[BEGIN_MST_COARSE] = aggregateGroup.groupTimeRange.minErt.coarse
        metadataTable[BEGIN_MST_FINE] = aggregateGroup.groupTimeRange.minErt.fine
        metadataTable[END_MST_COARSE] = aggregateGroup.groupTimeRange.maxErt.coarse
        metadataTable[END_MST_FINE] = aggregateGroup.groupTimeRange.maxErt.fine

        group = self.setAggregateMetadata(metadataTable, aggregateGroup, set_sclk_scet=False)

        metadataTable[CONTENTS] = self.prepareAggregateBlob(group)

        ldiRecord = "{},{},{},{},{},{},{},{},{},{},{},{},{},{},".format(
            metadataTable[HOST_ID],
            metadataTable[SESSION_ID],
            metadataTable[SESSION_FRAGMENT],
            self.recordIdProvider.getNextId(),
            metadataTable[BEGIN_RCT_COARSE],
            metadataTable[END_RCT_COARSE],
            metadataTable[BEGIN_MST_COARSE],
            metadataTable[BEGIN_MST_FINE],
            metadataTable[END_MST_COARSE],
            metadataTable[END_MST_FINE],
            metadataTable[DSS_ID],
            metadataTable[COUNT],
            metadataTable[DISTINCT_COUNT],
            metadataTable[CHAN_IDS_STRING]
        )
        ldiRecord += metadataTable[CONTENTS].replace(',', '\\,')
        return ldiRecord

    def getAggregateLdiRecord(self, aggregateGroup):
        '''Get the aggregate LDI record.
        Prepares the LDI record based on the Channel Type of the aggregate

        Params
        -------
        aggregateGroup - The aggregate group

        Returns
        --------
        The aggregate LDI record'''

        chanType = aggregateGroup.discriminator.channelType
        if chanType == ChannelType.FSW_RT or chanType == ChannelType.FSW_REC:
            ldiRecord = self.getChannelAggregateLdiRecord(aggregateGroup)
        elif (chanType == ChannelType.FSW_HEADER_FRAME
            or chanType == ChannelType.FSW_HEADER_PACKET
            or chanType == ChannelType.SSE_HEADER):
            ldiRecord = self.getHeaderChannelAggregateLdiRecord(aggregateGroup)
        elif chanType == ChannelType.SSE:
            ldiRecord = self.getSseChannelAggregateLdiRecord(aggregateGroup)
        elif chanType == ChannelType.MONITOR:
            ldiRecord = self.getMonitorChannelAggregateLdiRecord(aggregateGroup)
        else:
            # Unrecognized chanType, error
            raise RuntimeError("Error: Unrecognized chanType: %s" % chanType)
            # sys.exit("Error: Unrecognized chanType: %s" % chanType)

        return ldiRecord

    def startConversion(self):
        '''Start ChannelValue to Aggregate conversion

        Params
        -------
        querySqlMap - The map which contains the SQL statements for each Channel Type'''

        cursor = self.conn.cursor()

        outputDataHandler = None

        # Create the SQL LOAD file handler
        sqlFileHandler = SqlFileHandler(self.mission, self.outputDir)
        sqlFileHandler.openSqlFile()

        querySqlMapSorted = collections.OrderedDict(sorted(self.querySqlMap.items()))

        # Go through each Channel Type and execute the SQL query to retrieve the channel samples
        for channelType,sql in querySqlMapSorted.items():
            aggregateGroupTable = {}

            print("\nRunning Query for type: '%s'" % channelType)
            print("-----------------------------")
            if self.debugMode:
                print("%s = %s" % (channelType, sql))

            try:
                cursor.execute(sql)
            except MySQLdb.Error as e:
                print("Encountered unexpected MariaDb error:")
                raise
                # sys.exit(e)

            ldiFileRecCnt = 0
            totalRecCnt = 0
            aggCnt = 0
            self.recordIdProvider.resetId()

            print("Starting to Build Aggregates for type: '%s'" % channelType)
            startTime = currentMilliTime()

            firstRecord = True

            # Process the ChannelValue results
            for row in cursor:

                # If the Channel Type query returned data (results are not empty) then we want
                # to set up the Output Data Handler for that type and also add the LDI LOAD
                # command to the SQL file
                if firstRecord:
                    outputDataHandler = OutputDataHandler(self.mission, self.outputDir, channelTypeMap[channelType])
                    outputDataHandler.openLdiFile()

                    sqlFileHandler.writeLoadLdiCommand(channelType, outputDataHandler.getLdiFilePath())
                    firstRecord = False

                # Map column names and values in row
                keyValPair = dict(zip(columnNames, row))

                # Initialize the Group Discriminator
                keyObject = EhaGroupDiscriminator()
                keyObject.sessionId = keyValPair[SESSION_ID]
                keyObject.hostId = keyValPair[HOST_ID]
                keyObject.sessionFragment = keyValPair[SESSION_FRAGMENT]
                keyObject.vcid = keyValPair[VC_ID]
                keyObject.dssId = keyValPair[DSS_ID]
                keyObject.isRealtime = keyValPair[IS_REALTIME]
                keyObject.isFromSSE = keyValPair[FROM_SSE]

                if not self.extendedSCET and keyValPair[SCET_FINE] is not None:
                    keyValPair[SCET_FINE] = keyValPair[SCET_FINE] * MILLION_I

                if channelType == ChannelType.FSW:
                    if keyValPair[IS_REALTIME] == 1:
                        keyObject.channelType = ChannelType.FSW_RT
                    else:
                        keyObject.channelType = ChannelType.FSW_REC
                elif channelType == ChannelType.FSW_HEADER:
                    if keyValPair[PARENT_TYPE] == "FRAME":
                        keyObject.channelType = ChannelType.FSW_HEADER_FRAME
                    else:
                        keyObject.channelType = ChannelType.FSW_HEADER_PACKET
                else:
                    keyObject.channelType = channelType

                # If an Aggregate Group with the same discriminator is currenly in progress,
                # add this channel sample to that group.
                # Otherwise create a new aggregate group
                if keyObject in aggregateGroupTable:
                    aggregateGroup = aggregateGroupTable[keyObject]
                    aggregateGroup.addMember(keyValPair)
                else:
                    aggregateGroup = AggregateGroup(keyObject, keyValPair)
                    aggregateGroupTable[keyObject] = aggregateGroup

                # We have 200 samples in the aggregate, write LDI record
                if aggregateGroup.groupMemberCount == CHANNEL_SAMPLES_PER_AGGREGATE:
                    ldiRecord = self.getAggregateLdiRecord(aggregateGroup)
                    ldiRecord = ldiRecord.decode('latin1') if isinstance(ldiRecord, bytes) else ldiRecord
                    # outputDataHandler.writeLdiRecord(ldiRecord + "\n")
                    outputDataHandler.writeLdiRecord(ldiRecord + "\n")

                    # This aggregate has been written to the LDI file
                    # Remove it from the table.
                    del aggregateGroupTable[keyObject]
                    self.metadataTable.clear()
                    aggCnt+=1

                    ldiFileRecCnt += 1

                    # Each Channel Type is written to its specific LDI file but
                    # we don't want to write all aggregate LDI records to a single file even for a
                    # single type. May run into file size limits if the channel sample size
                    # is significantly large.
                    #
                    # Create a new LDI file once the Maximum number of Aggregate
                    # records per LDI file limit has been reached
                    if ((ldiFileRecCnt * CHANNEL_SAMPLES_PER_AGGREGATE) >= self.maxAggPerLdiFile):
                        outputDataHandler.incrementLdiFilePart()
                        sqlFileHandler.writeLoadLdiCommand(channelType, outputDataHandler.getLdiFilePath())
                        ldiFileRecCnt = 0

                # Report progress every 100,000 channel samples
                if totalRecCnt != 0 and totalRecCnt % 100000 == 0:

                    if self.debugMode:
                        if startTime != 0:
                            deltaTime = (currentMilliTime() - startTime)
                            seconds = float(float(deltaTime)/1000.0)
                            sampleRate = str(round((totalRecCnt / seconds), 2))
                            aggRate = str(round((aggCnt / seconds), 2))
                        else:
                            sampleRate = aggRate = 0

                        print("Total sample count = {}, aggregate count = {}, sample conversion rate = {} (samples/sec), agg build rate = {} (agg/sec)".format(
                            totalRecCnt, aggCnt, sampleRate, aggRate))
                    else:
                        print("Total sample count = {}, aggregate count = {}".format(
                            totalRecCnt, aggCnt))


                totalRecCnt += 1


            for disc, aggregateGroup in aggregateGroupTable.items():
                ldiRecord = self.getAggregateLdiRecord(aggregateGroup)
                ldiRecord = ldiRecord.decode('latin1') if isinstance(ldiRecord, bytes) else ldiRecord
                outputDataHandler.writeLdiRecord(ldiRecord + "\n")
                aggCnt+=1

            print("Total sample count for type '%s' = %d, aggregate count = %d" % (channelType, totalRecCnt, aggCnt))
            if outputDataHandler is not None and not outputDataHandler.isClosed():
                outputDataHandler.closeLdiFile()
            print("-----------------------------")

        # Close the LDI LOAD SQL File
        sqlFileHandler.closeSqlFile()


def test():
    # Initialize from command line options
    (mission,outputDir,databaseHost,databasePort,databaseUser,\
     databasePassword,databaseName,testKey,sessionRange,\
     maxAggPerLdiFile,extendedSCET,debugMode) = parseCommandLine()

    # Session option must be specified
    if not testKey or testKey == "":
        sys.exit('Session option --testKey/-K is required')

    if not databaseUser or databaseUser == "":
        databaseUser = "mpcs"

    # Database name must be specified
    if not databaseName or databaseName == "":
        sys.exit('Database Name is required')

    # Set the default host to localhost
    if databaseHost is None:
        databaseHost = "localhost"

    # Set a default port if not specified
    if databasePort is None:
        databasePort = 3306

    # Set an empty DB password if its not provided
    if databasePassword is None:
        databasePassword = ""

    print("======================")
    print("Mission: {}\nSessions: {}\nDatabase Host: {}\nDatabase Port: {}\nDatabase Name: {}".format(
        mission, testKey, databaseHost, databasePort, databaseName))
    print("Database User: {}\nDatabase Pass: {}\nOutput Directory: {}\nCHILL_GDS: {}".format(
        databaseUser, databasePassword, os.path.abspath(outputDir), os.environ.get('CHILL_GDS')))
    print("======================")

    _keys=['mission','outputDir','databaseHost','databasePort','databaseUser','databasePassword','databaseName','testKey','sessionRange','maxAggPerLdiFile','extendedSCET','debugMode']
    _values=[mission,outputDir,databaseHost,databasePort,databaseUser,databasePassword,databaseName,testKey,sessionRange,maxAggPerLdiFile,extendedSCET,debugMode]
    _handler=ConvHandler(**dict(zip(_keys, _values)))
    _handler()




def main():
    return test()

if __name__ == "__main__":
    main()

################################################################################
# vim:set sr et ts=4 sw=4 ft=python fenc=utf-8: // See Vim, :help 'modeline'
